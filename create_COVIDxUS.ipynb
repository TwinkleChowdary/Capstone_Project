{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Copyright\n", "Copyright (c) 2021, Her Majesty in Right of Canada as represented by the National Research Council Canada. Rights provided under GNU GENERAL PUBLIC LICENSE, Version 3. Full text of the license accessible at the [LICENSE](LICENSE) file.\n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This scripts automatically collect videos from the following sources, processes and curates them, and stores them along with their metadata on the user's device:\n", "\n", "* ButterflyNetwork\n", "* GrepMed\n", "* LITFL\n", "* The PocusAtlas\n", "* Radiopaedia\n", "* CoreUltrasound\n", "* University of Florida (UF)\n", "* Scientific Publications\n", "* Clarius\n", "\n", "In addition, it extracts images from the collected videos, processes and curates them, and stores images and their metadata locally on the user's device.\n", "\n", "__Note:__ No data is stored on the NRC-COVIDx-US repository and it only contains scripts to systematically collect, curate, and integrate data on user's device."]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Libraries"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import os\n", "import re\n", "import shutil\n", "import random \n", "\n", "import cv2\n", "from PIL import Image\n", "\n", "import zipfile\n", "from selenium import webdriver\n", "from selenium.webdriver.chrome.options import Options\n", "import requests\n", "from vimeo_downloader import Vimeo\n", "import urllib.request\n", "\n", "from progressbar import ProgressBar\n", "\n", "import time\n", "from image_data import extract_images\n", "\n", "import matplotlib.pyplot as plt\n", "\n", "import subprocess # to unzip butterfly file\n", "import glob"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Pandas 1.3.4\n", "selenium 3.141.0\n", "requests 2.26.0\n"]}], "source": ["print(\"Pandas\", pd.__version__)\n", "import selenium\n", "print(\"selenium\", selenium.__version__)\n", "print(\"requests\", requests.__version__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Functions"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["def get_download_path():\n", "    \"\"\"Returns the default downloads path for linux or windows\"\"\"\n", "    if os.name == 'nt':\n", "        import winreg\n", "        sub_key = r'SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders'\n", "        downloads_guid = '{374DE290-123F-4565-9164-39C4925E467B}'\n", "        with winreg.OpenKey(winreg.HKEY_CURRENT_USER, sub_key) as key:\n", "            location = winreg.QueryValueEx(key, downloads_guid)[0]\n", "        return location\n", "    else:\n", "        return os.path.join(os.path.expanduser('~'), 'downloads')\n", "    \n", "def remove_html_tags(text):\n", "    \"\"\"Function to remove html tags from a string\"\"\"\n", "    import re\n", "    clean = re.compile('<.*?>')\n", "    return re.sub(clean, '', text)\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# code to download zip files from Google drive, in case required\n", "def download_file_from_google_drive(id, destination):\n", "    URL = \"https://docs.google.com/uc?export=download\"\n", "\n", "    session = requests.Session()\n", "\n", "    response = session.get(URL, params = { 'id' : id }, stream = True)\n", "    token = get_confirm_token(response)\n", "\n", "    if token:\n", "        params = { 'id' : id, 'confirm' : token }\n", "        response = session.get(URL, params = params, stream = True)\n", "\n", "    save_response_content(response, destination)    \n", "\n", "def get_confirm_token(response):\n", "    for key, value in response.cookies.items():\n", "        if key.startswith('download_warning'):\n", "            return value\n", "\n", "    return None\n", "\n", "def save_response_content(response, destination):\n", "    CHUNK_SIZE = 32768\n", "    progress = ProgressBar() \n", "    \n", "    with open(destination, \"wb\") as f:\n", "        for chunk in progress(response.iter_content(CHUNK_SIZE)):\n", "            if chunk: # filter out keep-alive new chunks\n", "                f.write(chunk)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Parameters"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# set save path directory\n", "SAVE_PATH = 'data'\n", "\n", "# create data, video, and image folders, if they do not exist\n", "if not os.path.exists('data'):\n", "    os.makedirs('data')\n", "if not os.path.exists('data/video'):\n", "    os.makedirs('data/video')\n", "if not os.path.exists('data/image'):\n", "    os.makedirs('data/image')\n", "    \n", "# setting chrome driver\n", "chromedriver = \"utils/chromedriver.exe\" \n", "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n", "chrome_options = Options()\n", "chrome_options.add_argument(\"--headless\")\n", "\n", "# setting global vars\n", "VIDEO_PATH = 'data/video/'\n", "IMAGE_PATH = 'data/image/'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Read the Metadata File"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(244, 21)\n"]}, {"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>id</th>\n", "      <th>filename</th>\n", "      <th>filetype</th>\n", "      <th>folder</th>\n", "      <th>source</th>\n", "      <th>url</th>\n", "      <th>probe</th>\n", "      <th>class</th>\n", "      <th>class_on_website</th>\n", "      <th>version</th>\n", "      <th>...</th>\n", "      <th>type</th>\n", "      <th>patient</th>\n", "      <th>case_no</th>\n", "      <th>gender</th>\n", "      <th>age</th>\n", "      <th>comment</th>\n", "      <th>paper_link</th>\n", "      <th>paper_doi</th>\n", "      <th>license</th>\n", "      <th>link</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>1_butterfly_covid</td>\n", "      <td>Coalescing B lines.mp4</td>\n", "      <td>mp4</td>\n", "      <td>data\\tmp\\Butterfly\\B lines</td>\n", "      <td>Butterfly</td>\n", "      <td>https://butterflynetwork.getbynder.com/transfe...</td>\n", "      <td>Convex</td>\n", "      <td>COVID</td>\n", "      <td>NaN</td>\n", "      <td>1.0</td>\n", "      <td>...</td>\n", "      <td>lung</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2_butterfly_covid</td>\n", "      <td>Confluent B lines.mp4</td>\n", "      <td>mp4</td>\n", "      <td>data\\tmp\\Butterfly\\B lines</td>\n", "      <td>Butterfly</td>\n", "      <td>https://butterflynetwork.getbynder.com/transfe...</td>\n", "      <td>Convex</td>\n", "      <td>COVID</td>\n", "      <td>NaN</td>\n", "      <td>1.0</td>\n", "      <td>...</td>\n", "      <td>lung</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>2 rows \u00c3\u2014 21 columns</p>\n", "</div>"], "text/plain": ["                  id                filename filetype  \\\n", "0  1_butterfly_covid  Coalescing B lines.mp4      mp4   \n", "1  2_butterfly_covid   Confluent B lines.mp4      mp4   \n", "\n", "                       folder     source  \\\n", "0  data\\tmp\\Butterfly\\B lines  Butterfly   \n", "1  data\\tmp\\Butterfly\\B lines  Butterfly   \n", "\n", "                                                 url   probe  class  \\\n", "0  https://butterflynetwork.getbynder.com/transfe...  Convex  COVID   \n", "1  https://butterflynetwork.getbynder.com/transfe...  Convex  COVID   \n", "\n", "  class_on_website  version  ...  type patient case_no  gender age  comment  \\\n", "0              NaN      1.0  ...  lung     NaN     NaN     NaN NaN      NaN   \n", "1              NaN      1.0  ...  lung     NaN     NaN     NaN NaN      NaN   \n", "\n", "  paper_link paper_doi license link  \n", "0        NaN       NaN     NaN  NaN  \n", "1        NaN       NaN     NaN  NaN  \n", "\n", "[2 rows x 21 columns]"]}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": ["metadata = pd.read_csv('utils/video_metadata.csv', sep=',', encoding='latin1')\n", "print(metadata.shape)\n", "metadata.head(2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Get Ultrasound Videos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.1. ButterflyNetwork"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Note1:__ Depending on your system configuration the download button may not load in time. If you get an error, increase the sleep time in the following code:\n", "\n", "```python\n", "time.sleep(5)\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["__Note2:__ The below code block works with Chrome web browser and with ChromeDriver version 88 that is included in the utils folder. Depending on the version of your chrome browser, you may get a ChromeDriver version error. If occurs, please download the correct version of ChromeDriver based on the version of your chrome browser from this [link](https://chromedriver.chromium.org/downloads), and copy it to the utils folder."]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["...Downloading ButterflyNetwork zip file...\n", "...Extracting the video files...\n"]}, {"ename": "FileExistsError", "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\HEMANTH\\\\Downloads\\\\Published -20210112T164653Z-001.zip' -> 'C:\\\\Users\\\\HEMANTH\\\\Downloads\\\\Published-20210112T164653Z-001.zip'", "output_type": "error", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)", "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23884/2460561707.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'...Extracting the video files...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# extract the downloaded zip file and remove the zip file after extraction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_file_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utils/7z.exe\"\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m' x '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mzip_file_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' -o'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'data/tmp/Butterfly'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:\\\\Users\\\\HEMANTH\\\\Downloads\\\\Published -20210112T164653Z-001.zip' -> 'C:\\\\Users\\\\HEMANTH\\\\Downloads\\\\Published-20210112T164653Z-001.zip'"]}], "source": ["# zip file url\n", "butterfly_url = metadata[(metadata.source == 'Butterfly') & (metadata.date_added == 'Mar_2021')].url.unique()[0]\n", "print('...Downloading ButterflyNetwork zip file...')\n", "\n", "# simulatting button click to download the zip file\n", "browser = webdriver.Chrome(chromedriver) #, options=chrome_options)\n", "browser.get(butterfly_url)\n", "\n", "# Download button sometimes doesn't load in time to click. If such error occurring, increase sleep time\n", "time.sleep(5)\n", "\n", "browser.find_element_by_class_name('btn-primary').click() \n", "\n", "# path to the downloaded zip file\n", "zip_file_path = os.path.join(get_download_path(), 'Published -20210112T164653Z-001.zip')  # new version - checked March 9, 2021\n", "\n", "# wait till the zip file is downloaded\n", "while not os.path.exists(zip_file_path):\n", "    time.sleep(1)\n", "\n", "# create butterfly folder under video folder, if it does not exist\n", "if not os.path.exists('data/tmp/Butterfly'):\n", "    os.makedirs('data/tmp/Butterfly')\n", "time.sleep(2)\n", "\n", "\n", "print('...Extracting the video files...')\n", "# extract the downloaded zip file and remove the zip file after extraction\n", "os.rename(zip_file_path, zip_file_path.replace(' ', ''))\n", "subprocess.Popen(\"utils/7z.exe\" +' x ' + zip_file_path.replace(' ', '') + ' -o' + 'data/tmp/Butterfly',stdout=subprocess.PIPE)\n", "time.sleep(5)\n", "\n", "# copy files from subfolders to the video folder\n", "for root, dirs, files in os.walk('data/tmp/Butterfly/Published_'):  \n", "    for file in files:\n", "        if file.endswith(\".png\"):\n", "            continue\n", "        path_file = os.path.join(root,file)\n", "        shutil.copy2(path_file, 'data/video') \n", "\n", "# renaming extracted files to their ids\n", "progress = ProgressBar() \n", "for root, dirs, files in os.walk('data/video'):  \n", "    for file in progress(files):\n", "        if file.endswith(\".png\"):\n", "            continue\n", "        path_file = os.path.join(root,file)\n", "        file_id = metadata[metadata.filename == file].id.values[0] + '.mp4'\n", "        # rename the file to its id\n", "        os.rename(path_file, os.path.join(root,file_id))\n", "\n", "print('=== ButterflyNetwork video files extraction done! ===')        \n", "        \n", "# delete the tmp folder and its contents\n", "shutil.rmtree('data/tmp')\n", "        \n", "# remove the zip file\n", "os.remove(zip_file_path.replace(' ', ''))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Note:** If you get error running the above cell, uncomment and run the following code block instead:"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Downloading Butterfly zip file\n"]}, {"name": "stderr", "output_type": "stream", "text": ["| |#                                                  | 0 Elapsed Time: 0:00:00\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Download complete\n", "Extracting video files\n", "Extraction complete\n"]}], "source": ["import requests\n", "\n", "print('Downloading Butterfly zip file')\n", "file_id = '18I4N6lWdcUW618Qwr6Krsd1Rkn946Ag0' # sharable link id\n", "zip_file_path = os.path.join(get_download_path(), 'butterfly.zip')\n", "download_file_from_google_drive(file_id, zip_file_path)\n", "\n", "# unzip video files\n", "print('Download complete\\nExtracting video files')\n", "open_file = subprocess.Popen(\"utils/7z.exe\" +' x ' + zip_file_path + ' -o' + 'data/video',stdout=subprocess.PIPE)\n", "print('Extraction complete')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.2. GrepMed"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["  5% (1 of 20) |#                        | Elapsed Time: 0:00:00 ETA:   0:00:03"]}, {"name": "stdout", "output_type": "stream", "text": ["...Extracting the video files...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["100% (20 of 20) |########################| Elapsed Time: 0:00:03 Time:  0:00:03\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== GrepMed video files extraction done! ===\n"]}], "source": ["print('...Extracting the video files...')\n", "grepmed_df = metadata[metadata.source == 'GrepMed']\n", "\n", "progress = ProgressBar(max_value=grepmed_df.shape[0]) \n", "for idx, row in progress(grepmed_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    # write the video file to disk\n", "    vid = requests.get(row.url).content\n", "    with open(os.path.join('data/video/', filename), 'wb') as handler:\n", "        handler.write(vid)\n", "print('=== GrepMed video files extraction done! ===')        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.3. LITFL"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["\r", "  0% (0 of 63) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"]}, {"name": "stdout", "output_type": "stream", "text": ["...Extracting the video files...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["100% (63 of 63) |########################| Elapsed Time: 0:00:22 Time:  0:00:22\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== LITFL video files extraction done! ===\n"]}], "source": ["print('...Extracting the video files...')\n", "litfl_df = metadata[metadata.source == 'Litfl']\n", "\n", "progress = ProgressBar(max_value=litfl_df.shape[0]) \n", "for idx, row in progress(litfl_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    # write the video file to disk\n", "    vid = requests.get(row.url).content\n", "    with open(os.path.join('data/video/', filename), 'wb') as handler:\n", "        handler.write(vid)\n", "print('=== LITFL video files extraction done! ===')        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.4. The POCUS Atlas"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["\r", "  0% (0 of 32) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"]}, {"name": "stdout", "output_type": "stream", "text": ["...Extracting the video files...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["100% (32 of 32) |########################| Elapsed Time: 0:00:06 Time:  0:00:06\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== THEPocusAtlas video files extraction done! ===\n"]}], "source": ["print('...Extracting the video files...')\n", "pocus_df = metadata[metadata.source == 'PocusAtlas']\n", "\n", "progress = ProgressBar(max_value=pocus_df.shape[0]) \n", "for idx, row in progress(pocus_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    # write the video file to disk\n", "    vid = requests.get(row.url).content\n", "    with open(os.path.join('data/video/', filename), 'wb') as handler:\n", "        handler.write(vid)\n", "print('=== THEPocusAtlas video files extraction done! ===')        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.5. Radiopaedia"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["\r", "  0% (0 of 5) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--"]}, {"name": "stdout", "output_type": "stream", "text": ["...Extracting the video files...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["100% (5 of 5) |##########################| Elapsed Time: 0:00:01 Time:  0:00:01\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Radiopaedia video files extraction done! ===\n"]}], "source": ["print('...Extracting the video files...')\n", "radio_df = metadata[metadata.source == 'Radiopaedia']\n", "\n", "progress = ProgressBar(max_value=radio_df.shape[0]) \n", "for idx, row in progress(radio_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    # write the video file to disk\n", "    vid = requests.get(row.url).content\n", "    with open(os.path.join('data/video/', filename), 'wb') as handler:\n", "        handler.write(vid)\n", "print('=== Radiopaedia video files extraction done! ===')        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.6. CoreUltrasound"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["\r", "  0% (0 of 18) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"]}, {"name": "stdout", "output_type": "stream", "text": ["...Extracting the video files...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["157_core_other.mp4: 1294KB [00:00, 11629.72KB/s]                                                                       \n", "158_core_pneumonia.mp4: 3892KB [00:00, 23289.12KB/s]                                                                   \n", "159_core_other.mp4: 3847KB [00:00, 24074.05KB/s]                                                                       \n", "160_core_other.mp4: 3847KB [00:00, 23567.36KB/s]                                                                       \n", "161_core_other.mp4: 4805KB [00:00, 21292.12KB/s]                                                                       \n", "162_core_other.mp4: 4805KB [00:00, 26585.38KB/s]                                                                       \n", "174_core_covid.mp4: 1633KB [00:00, 13163.75KB/s]                                                                       \n", "100% (18 of 18) |########################| Elapsed Time: 0:00:16 Time:  0:00:16\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== CoreUltrasound video files extraction done! ===\n"]}], "source": ["print('...Extracting the video files...')\n", "core_df = metadata[metadata.source == 'CoreUltrasound']\n", "\n", "progress = ProgressBar(max_value=core_df.shape[0]) \n", "for idx, row in progress(core_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    \n", "    # extract videos from Vimeo\n", "    if 'vimeo' in row.url:\n", "        v = Vimeo(row.url)\n", "        stream = v.streams # List of available streams of different quality\n", "        highest_quality_available = stream[-1]\n", "        highest_quality_available.download(download_directory = 'data/video/', filename = filename.split('.')[0])\n", "    # extract mp4 videos\n", "    else:\n", "        # write the video file to disk\n", "        vid = requests.get(row.url).content\n", "        with open(os.path.join('data/video/', filename), 'wb') as handler:\n", "            handler.write(vid)\n", "print('=== CoreUltrasound video files extraction done! ===')        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.7. UF"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100% (2 of 2) |##########################| Elapsed Time: 0:00:08 Time:  0:00:08\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== 2 extra video files downloaded! ===\n"]}], "source": ["paper_df = metadata[(metadata.source == 'Paper') & ((metadata['id'].str.contains('199', na=False)) | (metadata['id'].str.contains('200', na=False)))] \n", "\n", "progress = ProgressBar(max_value=paper_df.shape[0]) \n", "for idx, row in progress(paper_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    \n", "    # write the video file to disk\n", "    r = requests.get(row.url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n", "    if r.status_code == 200:\n", "        with open(os.path.join('data/video/', filename), 'wb') as f:\n", "            r.raw.decode_content = True\n", "            shutil.copyfileobj(r.raw, f)       \n", "\n", "    # set a random delay, otherwise the connection gets closed\n", "    delay = random.randint(3, 5)\n", "    time.sleep(delay)\n", "print('=== 2 extra video files downloaded! ===')        "]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["\r", "  0% (0 of 24) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"]}, {"name": "stdout", "output_type": "stream", "text": ["...Extracting the video files...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["100% (24 of 24) |########################| Elapsed Time: 0:02:32 Time:  0:02:32\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== UF video files extraction done! ===\n"]}], "source": ["print('...Extracting the video files...')\n", "uf_df = metadata[metadata.source == 'UF']\n", "\n", "progress = ProgressBar(max_value=uf_df.shape[0]) \n", "for idx, row in progress(uf_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    \n", "    # write the video file to disk\n", "    r = requests.get(row.url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n", "    if r.status_code == 200:\n", "        with open(os.path.join('data/video/', filename), 'wb') as f:\n", "            r.raw.decode_content = True\n", "            shutil.copyfileobj(r.raw, f)       \n", "\n", "    # set a random delay, otherwise the connection gets closed\n", "    delay = random.randint(3, 5)\n", "    time.sleep(delay)\n", "print('=== UF video files extraction done! ===')        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.8. Scientific Publications"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["\r", "  0% (0 of 22) |                         | Elapsed Time: 0:00:00 ETA:  --:--:--"]}, {"name": "stdout", "output_type": "stream", "text": ["...Extracting the video files...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["100% (22 of 22) |########################| Elapsed Time: 0:02:15 Time:  0:02:15\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Video files extraction from papers is done! ===\n"]}], "source": ["print('...Extracting the video files...')\n", "paper_df = metadata[(metadata.source == 'Paper')] \n", "\n", "progress = ProgressBar(max_value=paper_df.shape[0]) \n", "for idx, row in progress(paper_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    \n", "    r = requests.get(row.url, stream=True, headers={'User-agent': 'Mozilla/5.0'})\n", "    if r.status_code == 200:\n", "        with open(os.path.join('data/video/', filename), 'wb') as f:\n", "            r.raw.decode_content = True\n", "            shutil.copyfileobj(r.raw, f)       \n", "        \n", "    # set a random delay, otherwise the connection gets closed\n", "    if (('241_' in row.id) | ('242_' in row.id) | ('243_' in row.id)): #longer delay for last files\n", "        delay = random.randint(10, 20)\n", "    else:\n", "        delay = random.randint(3, 5)\n", "    time.sleep(delay)\n", "print('=== Video files extraction from papers is done! ===')        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1.9. Clarius\n", "* Extracting the first part of Clarius files (**6 files**)"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["\r", "  0% (0 of 6) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--"]}, {"name": "stdout", "output_type": "stream", "text": ["...Extracting the video files...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["100% (6 of 6) |##########################| Elapsed Time: 0:00:27 Time:  0:00:27\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Clarius video files extraction done! ===\n"]}], "source": ["print('...Extracting the video files...')\n", "clarius_df = metadata[metadata.source == 'Clarius'].iloc[:6, :]\n", "\n", "progress = ProgressBar(max_value=clarius_df.shape[0]) \n", "for idx, row in progress(clarius_df.iterrows()):\n", "    filename = row.id + '.' + row.filetype\n", "    \n", "    # write the video file to disk\n", "    r = requests.get(row.url, stream=True, headers={'User-agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64)'})\n", "    if r.status_code == 200:\n", "        with open(os.path.join('data/video/', filename), 'wb') as f:\n", "            r.raw.decode_content = True\n", "            shutil.copyfileobj(r.raw, f)       \n", "\n", "    # set a random delay, otherwise the connection gets closed\n", "    delay = random.randint(3, 5)\n", "    time.sleep(delay)\n", "print('=== Clarius video files extraction done! ===')        "]}, {"cell_type": "markdown", "metadata": {}, "source": ["* Extracting the second part of Clarius files (**17 files**)"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Downloading Clarius zip file...\n"]}, {"name": "stderr", "output_type": "stream", "text": ["| |#                                                  | 0 Elapsed Time: 0:00:00\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Clarius video files extraction done! ===\n"]}], "source": ["import requests\n", "\n", "print('Downloading Clarius zip file...')\n", "file_id = '1bqsqNzAJYwdriOP9CcGWPzCUB-7G72Ta' # sharable link id\n", "zip_file_path = os.path.join(get_download_path(), 'clarius.zip')\n", "download_file_from_google_drive(file_id, zip_file_path)\n", "\n", "# unzip video files\n", "open_file = subprocess.Popen(\"utils/7z.exe\" +' x ' + zip_file_path + ' -o' + 'data/video',stdout=subprocess.PIPE)\n", "print('=== Clarius video files extraction done! ===')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Video Preprocessing"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Move original video files to the original folder"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100% (1 of 1) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"]}], "source": ["source_dir = 'data/video/'\n", "target_dir = 'data/video/original'\n", "    \n", "file_names = os.listdir(source_dir)\n", "\n", "if not os.path.exists('data/video/original'): \n", "    os.makedirs('data/video/original') \n", "\n", "progress = ProgressBar()\n", "for file_name in progress(file_names):\n", "    shutil.move(os.path.join(source_dir, file_name), target_dir)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2.1. Fetching Video Files Properties"]}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100% (244 of 244) |######################| Elapsed Time: 0:00:07 Time:  0:00:07\n"]}], "source": ["VIDEO_PATH_ORG = 'data/video/original/'\n", "\n", "vid_files = os.listdir(VIDEO_PATH_ORG)\n", "\n", "progress = ProgressBar(max_value=metadata.shape[0]) \n", "with open('utils/video_files_properties.csv', 'w') as f:\n", "    # write the file header\n", "    f.write('filename,framerate,width,height,frame_count,duration_secs\\n')\n", "    \n", "    # loop over the video files and get their properties\n", "    for vid in progress(vid_files):\n", "        vid_filename = VIDEO_PATH_ORG + str(vid)\n", "        file_type = vid.split('.')[-1]\n", "        \n", "        # get video file properties\n", "        cv2video = cv2.VideoCapture(vid_filename)\n", "        height = cv2video.get(cv2.CAP_PROP_FRAME_HEIGHT)\n", "        width  = cv2video.get(cv2.CAP_PROP_FRAME_WIDTH) \n", "        frame_rate = round(cv2video.get(cv2.CAP_PROP_FPS), 2)\n", "        frame_count=0\n", "        duration=0\n", "        if file_type == 'mp4':\n", "            frame_count = cv2video.get(cv2.CAP_PROP_FRAME_COUNT) \n", "            duration = round((frame_count / frame_rate), 2)\n", "        elif file_type == 'gif':\n", "            frame_count = round(Image.open(vid_filename).n_frames) #round((duration * frame_rate ), 0)\n", "            duration = round((frame_count / frame_rate), 2)\n", "\n", "        # write video properties to the file\n", "        line_to_write = str(vid) + ',' + str(frame_rate) + ',' + str(width) + ',' + str(height) + ',' + str(frame_count) + ',' + str(duration) + '\\n'\n", "        f.write(line_to_write)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2.2. Video Preprocessing"]}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": ["VIDEO_CROPPED_OUT = 'data/video/cropped/' #processed/cropped/'\n", "\n", "# create processed and cropped folder if they don't already exist\n", "if not os.path.exists('data/video/cropped'): #processed/cropped'):\n", "    os.makedirs('data/video/cropped') #processed/cropped')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2.1. Inital Cropping"]}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(243, 27)\n"]}, {"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>filename</th>\n", "      <th>source</th>\n", "      <th>probe</th>\n", "      <th>class</th>\n", "      <th>org_width</th>\n", "      <th>org_height</th>\n", "      <th>org_framecount</th>\n", "      <th>org_framerate</th>\n", "      <th>org_duration</th>\n", "      <th>green_dot</th>\n", "      <th>...</th>\n", "      <th>del_upper</th>\n", "      <th>width_rate</th>\n", "      <th>x1_w_y1_h</th>\n", "      <th>cropped_filename</th>\n", "      <th>crp_width</th>\n", "      <th>crp_height</th>\n", "      <th>version</th>\n", "      <th>date_added</th>\n", "      <th>multiple_videos</th>\n", "      <th>Note</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>1_butterfly_covid.mp4</td>\n", "      <td>Butterfly</td>\n", "      <td>Convex</td>\n", "      <td>COVID</td>\n", "      <td>880</td>\n", "      <td>1080</td>\n", "      <td>65</td>\n", "      <td>19.57</td>\n", "      <td>3.32</td>\n", "      <td>no</td>\n", "      <td>...</td>\n", "      <td>15.0</td>\n", "      <td>0.035</td>\n", "      <td>NaN</td>\n", "      <td>1_butterfly_covid_prc.avi</td>\n", "      <td>820.0</td>\n", "      <td>820.0</td>\n", "      <td>1.0</td>\n", "      <td>Nov_2020</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2_butterfly_covid.mp4</td>\n", "      <td>Butterfly</td>\n", "      <td>Convex</td>\n", "      <td>COVID</td>\n", "      <td>720</td>\n", "      <td>1236</td>\n", "      <td>818</td>\n", "      <td>30.00</td>\n", "      <td>27.27</td>\n", "      <td>yes</td>\n", "      <td>...</td>\n", "      <td>83.0</td>\n", "      <td>0.068</td>\n", "      <td>NaN</td>\n", "      <td>2_butterfly_covid_prc.avi</td>\n", "      <td>624.0</td>\n", "      <td>624.0</td>\n", "      <td>1.0</td>\n", "      <td>Nov_2020</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>2 rows \u00c3\u2014 27 columns</p>\n", "</div>"], "text/plain": ["                filename     source   probe  class  org_width  org_height  \\\n", "0  1_butterfly_covid.mp4  Butterfly  Convex  COVID        880        1080   \n", "1  2_butterfly_covid.mp4  Butterfly  Convex  COVID        720        1236   \n", "\n", "   org_framecount  org_framerate  org_duration green_dot  ... del_upper  \\\n", "0              65          19.57          3.32        no  ...      15.0   \n", "1             818          30.00         27.27       yes  ...      83.0   \n", "\n", "  width_rate x1_w_y1_h           cropped_filename crp_width crp_height  \\\n", "0      0.035       NaN  1_butterfly_covid_prc.avi     820.0      820.0   \n", "1      0.068       NaN  2_butterfly_covid_prc.avi     624.0      624.0   \n", "\n", "  version  date_added  multiple_videos Note  \n", "0     1.0    Nov_2020              NaN  NaN  \n", "1     1.0    Nov_2020              NaN  NaN  \n", "\n", "[2 rows x 27 columns]"]}, "execution_count": 26, "metadata": {}, "output_type": "execute_result"}], "source": ["# read cropping metadata file\n", "vid_crp_metadata = pd.read_csv('utils/video_cropping_metadata.csv', sep=',', encoding='latin1')\n", "print(vid_crp_metadata.shape)\n", "vid_crp_metadata.head(2)"]}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100% (243 of 243) |######################| Elapsed Time: 0:01:19 Time:  0:01:19\n"]}, {"name": "stdout", "output_type": "stream", "text": ["Initial cropping done...\n"]}], "source": ["progress = ProgressBar(max_value=vid_crp_metadata.shape[0])\n", "\n", "for idx, row in progress(vid_crp_metadata.iterrows()):\n", "    vid_arr = []  # array to store frames of a video file\n", "    \n", "    filename = row.filename\n", "    file_label = filename.split('_')[-1].split('.')[0] # label of the video file\n", "    \n", "    # the following file was removed in the new release of butterfly data\n", "    if filename == '22_butterfly_covid.mp4':\n", "        continue\n", "    \n", "    cap = cv2.VideoCapture(os.path.join(VIDEO_PATH_ORG, filename))\n", "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) + 0.5)\n", "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) + 0.5)\n", "    dim = (width, height) # dimension of the original file\n", "    \n", "    if pd.isna(row.x1_w_y1_h): # square cropping\n", "        DEL_UPPER = int(row.del_upper) # to remove top\n", "        WIDTH_RATE = float(row.width_rate) # to remove sides e.g. the meter\n", "        \n", "        width_border = int(width * WIDTH_RATE)\n", "        width_box = int(width - (2 * width_border)) \n", "        if width_box + DEL_UPPER > height:\n", "            width_box = int(height - DEL_UPPER)\n", "            width_border = int( (width / 2) - (width_box / 2))\n", "\n", "        while(True):\n", "            ret, frame = cap.read()\n", "\n", "            if not ret:\n", "                break\n", "\n", "            # crop\n", "            frame = frame[DEL_UPPER:width_box + DEL_UPPER, width_border:width_box + width_border]\n", "\n", "            frame = np.asarray(frame).astype(np.uint8)\n", "            vid_arr.append(frame)\n", "\n", "    else: # crop using (x1,y1) and (x2, y2). The output will not be necessarily a square file\n", "        X1 = int(row.x1_w_y1_h.split(',')[0].replace('(', ''))\n", "        W = int(row.x1_w_y1_h.split(',')[1].strip())\n", "        Y1 = int(row.x1_w_y1_h.split(',')[2].strip())\n", "        H = int(row.x1_w_y1_h.split(',')[3].replace(')', '').strip())\n", "\n", "        while(True):\n", "            ret, frame = cap.read()\n", "\n", "            if not ret:\n", "                break\n", "\n", "            # crop\n", "            frame = frame[Y1:Y1 + H, X1:X1 + W]\n", "\n", "            frame = np.asarray(frame).astype(np.uint8)\n", "            vid_arr.append(frame)\n", "\n", "    vid_arr = np.asarray(vid_arr)\n", "    # print(\"vid_arr.shape {}\".format(vid_arr.shape))\n", "    if (len(vid_arr.shape) != 4):\n", "        continue\n", "    prc_dim = vid_arr.shape[1:3] # dimension of the cropped file\n", "    prc_dim = (prc_dim[1], prc_dim[0])\n", "\n", "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n", "    out = cv2.VideoWriter(os.path.join(VIDEO_CROPPED_OUT + filename.split('.')[0] + '_prc.avi'), fourcc, 20.0, tuple(prc_dim))\n", "\n", "    for frame in vid_arr:\n", "        out.write(frame.astype(\"uint8\"))\n", "\n", "    vid_crp_metadata.iloc[idx, vid_crp_metadata.columns.get_loc('crp_width')] = prc_dim[1]\n", "    vid_crp_metadata.iloc[idx, vid_crp_metadata.columns.get_loc('crp_height')] = prc_dim[0]\n", "\n", "    cap.release()\n", "    out.release()\n", "    cv2.destroyAllWindows()\n", "\n", "vid_crp_metadata.to_csv('utils/video_cropping_metadata.csv', index=None)\n", "\n", "print('Initial cropping done...')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Extract Ultrasound Images from Videos"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Read video properties"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(188, 9)\n"]}, {"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>filename</th>\n", "      <th>framerate</th>\n", "      <th>width</th>\n", "      <th>height</th>\n", "      <th>frame_count</th>\n", "      <th>duration_secs</th>\n", "      <th>source</th>\n", "      <th>probe</th>\n", "      <th>class</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>100_litfl_other.mp4</td>\n", "      <td>15.0</td>\n", "      <td>480.0</td>\n", "      <td>360.0</td>\n", "      <td>46.0</td>\n", "      <td>3.07</td>\n", "      <td>Litfl</td>\n", "      <td>Convex</td>\n", "      <td>Other</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>101_litfl_other.mp4</td>\n", "      <td>15.0</td>\n", "      <td>480.0</td>\n", "      <td>360.0</td>\n", "      <td>28.0</td>\n", "      <td>1.87</td>\n", "      <td>Litfl</td>\n", "      <td>Convex</td>\n", "      <td>Other</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["              filename  framerate  width  height  frame_count  duration_secs  \\\n", "0  100_litfl_other.mp4       15.0  480.0   360.0         46.0           3.07   \n", "1  101_litfl_other.mp4       15.0  480.0   360.0         28.0           1.87   \n", "\n", "  source   probe  class  \n", "0  Litfl  Convex  Other  \n", "1  Litfl  Convex  Other  "]}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": ["vid_prop_df = pd.read_csv('utils/video_files_properties.csv')\n", "\n", "# merge with the video meta data file \n", "vid_prop_df.filename = vid_prop_df.filename.astype(str)\n", "vid_prop_df.filename = vid_prop_df.filename.str.strip()\n", "\n", "metadata['filename2'] = metadata.id + '.' + metadata.filetype\n", "metadata.filename2 = metadata.filename2.astype(str)\n", "metadata.filename2 = metadata.filename2.str.strip()\n", "\n", "vid_prop_df = pd.merge(vid_prop_df, metadata[['filename2', 'source', 'probe', 'class']], left_on='filename', right_on='filename2', how='left').drop('filename2', axis=1)\n", "\n", "del metadata['filename2']\n", "print(vid_prop_df.shape)\n", "vid_prop_df.head(2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Extract frames from original videos\n", "* v1.4.: 32,052 images are extracted\n", "* v1.3.: 19,161 images are extracted\n", "* v1.2.: 15,282 images are extracted"]}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": ["IMAGE_PATH_ORG = 'data/image/original/'\n", "\n", "# create a folder for images extracted from original videos, if doesn't exist\n", "if not os.path.exists(IMAGE_PATH_ORG):\n", "    os.makedirs(IMAGE_PATH_ORG)"]}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100% (188 of 188) |######################| Elapsed Time: 0:02:47 Time:  0:02:47\n"]}], "source": ["extract_images(video_path= VIDEO_PATH_ORG, image_path=IMAGE_PATH_ORG, cropped=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.1. Extract frames from cropped video files"]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": ["IMAGE_CROPPED_OUT = 'data/image/cropped/'\n", "IMAGE_MASK_OUT = 'data/mask/'\n", "\n", "# create cropped and inpainted image folders and the mask folder if they don't already exist\n", "if not os.path.exists(IMAGE_CROPPED_OUT):\n", "    os.makedirs(IMAGE_CROPPED_OUT)\n", "if not os.path.exists(IMAGE_MASK_OUT):\n", "    os.makedirs(IMAGE_MASK_OUT)"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100% (243 of 243) |######################| Elapsed Time: 0:01:41 Time:  0:01:41\n"]}], "source": ["extract_images(video_path= VIDEO_CROPPED_OUT, image_path=IMAGE_CROPPED_OUT, cropped=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1.1. (Optional) Extracting frames from cropped ultrasouund video files using a parameter set as filter\n", "* You can extract images using the follwoing parameters:\n", "    * maximum number of frames to be extracted from each video file\n", "    * extracting a targetted set of classes from ['COVID', 'Pneumonia', 'Normal', 'Other']\n", "    * extracting a targetted set of data sources from ['Butterfly', 'GrepMed', 'LITFL', 'PocusAtlas', 'CU', 'Radiopaedia', 'UF', 'Paper', 'Clarius']\n", "    * extracting a targetted set of probes from ['convex', 'linear']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#extract_images(video_path= VIDEO_CROPPED_OUT, image_path=IMAGE_CROPPED_OUT, cropped=True, \n", "#                max_frames=10, \n", "#                target_class=['COVID', 'Pneumonia', 'Normal'],\n", "#                target_source=['Butterfly', 'GrepMed', 'LITFL', 'PocusAtlas'],\n", "#                target_probe=['convex', 'linear']))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.2. Preprocessing Images"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Read image preprocessing metadata"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["(242, 17)\n"]}, {"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>filename</th>\n", "      <th>source</th>\n", "      <th>probe</th>\n", "      <th>class</th>\n", "      <th>org_width</th>\n", "      <th>org_height</th>\n", "      <th>cropped_filename</th>\n", "      <th>crp_width</th>\n", "      <th>crp_height</th>\n", "      <th>need_mask_after_crop</th>\n", "      <th>need_multiple_masks</th>\n", "      <th>frame_specific_masks</th>\n", "      <th>delete_frames_from_to</th>\n", "      <th>mask_main_filename</th>\n", "      <th>tight_inpainting</th>\n", "      <th>version</th>\n", "      <th>date_added</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>1_butterfly_covid.mp4</td>\n", "      <td>Butterfly</td>\n", "      <td>Convex</td>\n", "      <td>COVID</td>\n", "      <td>880</td>\n", "      <td>1080</td>\n", "      <td>1_butterfly_covid_prc.avi</td>\n", "      <td>820</td>\n", "      <td>820</td>\n", "      <td>yes</td>\n", "      <td>no</td>\n", "      <td>NaN</td>\n", "      <td>NaN</td>\n", "      <td>1_butterfly_covid_prc_convex_frame0_mask.jpg</td>\n", "      <td>no</td>\n", "      <td>1.0</td>\n", "      <td>Nov_2020</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>2_butterfly_covid.mp4</td>\n", "      <td>Butterfly</td>\n", "      <td>Convex</td>\n", "      <td>COVID</td>\n", "      <td>720</td>\n", "      <td>1236</td>\n", "      <td>2_butterfly_covid_prc.avi</td>\n", "      <td>624</td>\n", "      <td>624</td>\n", "      <td>yes</td>\n", "      <td>yes</td>\n", "      <td>118-130, 134-139, 147-150</td>\n", "      <td>131-133, 143-146, 154-202, 210-813</td>\n", "      <td>2_butterfly_covid_prc_convex_frame0_mask.jpg</td>\n", "      <td>no</td>\n", "      <td>1.0</td>\n", "      <td>Nov_2020</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>"], "text/plain": ["                filename     source   probe  class  org_width  org_height  \\\n", "0  1_butterfly_covid.mp4  Butterfly  Convex  COVID        880        1080   \n", "1  2_butterfly_covid.mp4  Butterfly  Convex  COVID        720        1236   \n", "\n", "            cropped_filename  crp_width  crp_height need_mask_after_crop  \\\n", "0  1_butterfly_covid_prc.avi        820         820                  yes   \n", "1  2_butterfly_covid_prc.avi        624         624                  yes   \n", "\n", "  need_multiple_masks       frame_specific_masks  \\\n", "0                  no                        NaN   \n", "1                 yes  118-130, 134-139, 147-150   \n", "\n", "                delete_frames_from_to  \\\n", "0                                 NaN   \n", "1  131-133, 143-146, 154-202, 210-813   \n", "\n", "                             mask_main_filename tight_inpainting  version  \\\n", "0  1_butterfly_covid_prc_convex_frame0_mask.jpg               no      1.0   \n", "1  2_butterfly_covid_prc_convex_frame0_mask.jpg               no      1.0   \n", "\n", "  date_added  \n", "0   Nov_2020  \n", "1   Nov_2020  "]}, "execution_count": 33, "metadata": {}, "output_type": "execute_result"}], "source": ["image_prc_df = pd.read_csv('utils/mask_metadata.csv')\n", "\n", "image_prc_df = image_prc_df[image_prc_df.filename !='22_butterfly_covid.mp4'] # 22_butterfly_covid.mp4 was removed in March release of butterfly\n", "\n", "print(image_prc_df.shape)\n", "image_prc_df.head(2)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2.1. Removing frames with artifacts on the ROI\n", "Some frames of the following files need to be deleted as the moving pointer is on ROI, we will remove them from the cropped images folder:\n", "* 2_butterfly_covid.mp4\n", "* 6_butterfly_covid.mp4\n", "* 16_butterfly_covid.mp4\n", "* 20_butterfly_normal.mp4\n", "* 22_butterfly_covid.mp4 (it was removed in the March release of butterfly data)\n", "* 25_grepmed_pneumonia.mp4\n", "* 178_uf_other.mp4 (initial 30 frames are removed)\n", "* 184_uf_pneumonia.mp4 (initial 30 frames are removed)\n", "\n", "We need 2 masks for the following videos:\n", "* 178_uf_other.mp4 \n", "* 184_uf_pneumonia.mp4\n", "\n", "**Number of frames:**\n", "* __Initial total number of frames:__ \n", "    * v1.4.: 32,052\n", "    * v1.3.: 19,161\n", "    * v1.2.: 13,646\n", "* __Total number of frames after removal:__ \n", "    * v1.4.: 29,651\n", "    * v1.3.: 16,822\n", "    * v1.2.: 11,307"]}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["100% (7 of 7) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"]}, {"name": "stdout", "output_type": "stream", "text": ["=== Files removed! ===\n"]}], "source": ["progress = ProgressBar(max_value=image_prc_df[~pd.isna(image_prc_df.delete_frames_from_to)].shape[0])\n", "\n", "for idx, row in progress(image_prc_df[~pd.isna(image_prc_df.delete_frames_from_to)].iterrows()):\n", "    frames_to_delete = row.delete_frames_from_to.strip().split(',')\n", "    frame_name_main = row.mask_main_filename.split('.')[0].split('_frame')[0]\n", "    \n", "    for frames in frames_to_delete:\n", "        from_frame = int(frames.split('-')[0])\n", "        to_frame = int(frames.split('-')[1]) + 1\n", "        \n", "        # delete frames with moving part on the roi\n", "        for i in range(from_frame, to_frame):\n", "            file_to_remove = IMAGE_CROPPED_OUT + frame_name_main + '_frame' + str(i) + '.jpg'\n", "            if os.path.exists(file_to_remove):\n", "                os.remove(file_to_remove)\n", "\n", "print(\"=== Files removed! ===\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2.2. Applying masks"]}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": ["CLEAN_IMAGE_OUT = 'data/image/clean/'\n", "CLEAN_VIDEO_OUT = 'data/video/clean/'\n", "\n", "# create clean image and video folders if they don't already exist\n", "if not os.path.exists(CLEAN_IMAGE_OUT):\n", "    os.makedirs(CLEAN_IMAGE_OUT)\n", "if not os.path.exists(CLEAN_VIDEO_OUT):\n", "    os.makedirs(CLEAN_VIDEO_OUT)"]}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [], "source": ["def zero_pad_array(arr, pad=5):\n", "    if len(arr.shape) == 3:\n", "        padded_arr = np.zeros((arr.shape[0]+2*pad, arr.shape[1]+2*pad, arr.shape[2]), dtype=np.uint8)\n", "        padded_arr[pad:pad + arr.shape[0], pad:pad + arr.shape[1], :] = arr\n", "    else:\n", "        padded_arr = np.zeros((arr.shape[0]+2*pad, arr.shape[1]+2*pad), dtype=np.uint8)\n", "        padded_arr[pad:pad + arr.shape[0], pad:pad + arr.shape[1]] = arr\n", "    return padded_arr\n", "        \n", "\n", "def frame_inpainting(frame_dict, mask, default_mask=0, kernel_size=(5,5), method='telea', pad=5):\n", "    '''\n", "    The function performs inpainting on frames using the created masks\n", "    \n", "    - frame_dict: dict of frames from video, indexed by frame number\n", "    - mask: (h, w, 1) array if single mask, else dict of such arrays\n", "        indexed by frame number\n", "    - default_mask: index for mask to be used as default, for frames\n", "        without specific mask (if mask is not constant across frames)\n", "    - kernel_size: Size of patch used to perform inpainting\n", "    - method: one of 'ns' (navier-stokes) or 'telea' - telea usually works better\n", "    '''\n", "    # Dilate mask make sure it covers enough of the ROI to be masked\n", "    kernel = np.ones(kernel_size, np.uint8)\n", "    if type(mask) is not dict:\n", "        mask = {default_mask: mask}\n", "    masks_processed = {key:cv2.dilate(zero_pad_array(m, pad=pad), kernel, iterations=1) for key, m in mask.items()}\n", "    \n", "    method_dict = {'ns':cv2.INPAINT_NS, 'telea':cv2.INPAINT_TELEA}\n", "    \n", "    frames_inpainted = {}\n", "    for key, frame in frame_dict.items():\n", "        if key in masks_processed:\n", "            #print(frame.shape, masks_processed[key].shape)\n", "            frames_inpainted[key] = cv2.inpaint(zero_pad_array(frame, pad=pad), masks_processed[key], 3, method_dict[method])[pad:-pad, pad:-pad, :]\n", "        else: # default mask\n", "            frames_inpainted[key] = cv2.inpaint(zero_pad_array(frame, pad=pad), masks_processed[default_mask], 3, method_dict[method])[pad:-pad, pad:-pad, :]\n", "    print(frames_inpainted)\n", "    return frames_inpainted"]}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": ["\r", "  0% (0 of 242) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"]}, {"name": "stdout", "output_type": "stream", "text": ["{}\n", "0\n"]}, {"ename": "IndexError", "evalue": "list index out of range", "output_type": "error", "traceback": ["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)", "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23884/4288144245.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mclean_vid_frames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mframes_inpainted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_vid_frames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_vid_frames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", "\u001b[1;31mIndexError\u001b[0m: list index out of range"]}], "source": ["progress = ProgressBar(max_value=image_prc_df.shape[0])\n", "\n", "for idx, row in progress(image_prc_df.iterrows()):     \n", "    # get the main token of the filename\n", "    if row.probe == 'Convex':\n", "        filename_main = row.filename.split('.')[0] + '_prc_convex'\n", "    elif row.probe == 'Linear':\n", "        filename_main = row.filename.split('.')[0] + '_prc_linear'\n", "        \n", "    if row.tight_inpainting == 'yes':\n", "        # objects close to ROI, avoid bleeding while inpainting\n", "        inpainting_kernel_size = (1,1)\n", "    else:\n", "        # no objects close to ROI, more effective inpainting\n", "        inpainting_kernel_size = (5,5)\n", "\n", "    # check if the cropped frames need cleaning\n", "    if row.need_mask_after_crop == 'no':\n", "        frames = {}\n", "        \n", "        # 1. no clearning, copy cropped images and rename them to clean folder\n", "        for file in os.listdir(IMAGE_CROPPED_OUT):\n", "            if file.startswith(filename_main):\n", "                #last_part = file.split('_')[-1]\n", "                #last_part = last_part.replace('frame', '_clean_frame')\n", "                new_filename = file.replace('frame', 'clean_frame')\n", "                #print(file, new_filename) #, last_part)\n", "                shutil.copy(IMAGE_CROPPED_OUT + file, CLEAN_IMAGE_OUT + new_filename)\n", "\n", "                img = cv2.imread(os.path.join(CLEAN_IMAGE_OUT, new_filename))\n", "                frame_num = int(re.search(r'\\d+$', file[:-4]).group())\n", "                frames[frame_num] = img\n", "        \n", "        # make a video out of the clean frames\n", "        keys = list(frames.keys())\n", "        keys.sort()\n", "        clean_vid_frames = [frames[k] for k in keys]\n", "\n", "        h, w, layers = clean_vid_frames[0].shape\n", "        size = (w, h)\n", "\n", "        out = cv2.VideoWriter(os.path.join(CLEAN_VIDEO_OUT + filename_main + '_clean.avi'), cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n", "        for i in range(len(clean_vid_frames)):\n", "            out.write(clean_vid_frames[i])\n", "        out.release()    \n", "        \n", "    # 2. frames need cleaning\n", "    else: \n", "        # create a dictionary of frames\n", "        frames = {}\n", "        for f in os.listdir(IMAGE_CROPPED_OUT):\n", "            if f.startswith(filename_main):\n", "                img = cv2.imread(os.path.join(IMAGE_CROPPED_OUT, f))\n", "                frame_num = int(re.search(r'\\d+$', f[:-4]).group())\n", "                frames[frame_num] = img\n", "\n", "        # check if the video file requires multiple masks or a single mask is enough\n", "        if row.need_multiple_masks == 'no':\n", "            mask = cv2.imread(os.path.join(IMAGE_MASK_OUT, filename_main + '_frame0_mask.jpg'))[:,:,0]\n", "\n", "            # perform inpainting on frames using a single main mask\n", "            frames_inpainted = frame_inpainting(frames, mask, kernel_size=inpainting_kernel_size)\n", "        else:\n", "            masks = {}\n", "           \n", "            for f in os.listdir(IMAGE_MASK_OUT):\n", "                if f.startswith(filename_main):\n", "                    img = cv2.imread(os.path.join(IMAGE_MASK_OUT, f))\n", "                    frame_num = int(re.search(r'\\d+$', f[:-9]).group())\n", "                    masks[frame_num] = img[:,:,0]\n", "\n", "            # perform inpainting on frames using multiple masks\n", "            frames_inpainted = frame_inpainting(frames, masks, default_mask=0, kernel_size=inpainting_kernel_size)\n", "\n", "        # write clean frames to the disk\n", "        for key, value in frames_inpainted.items():\n", "            cv2.imwrite(CLEAN_IMAGE_OUT + filename_main + \"_clean_frame\" + str(key) + \".jpg\", value)\n", "\n", "        # write clean video to the disk\n", "        keys = list(frames_inpainted.keys())\n", "        keys.sort()\n", "        clean_vid_frames = [frames_inpainted[k] for k in keys]\n", "        print(len(clean_vid_frames))\n", "        h, w, layers = clean_vid_frames[0].shape\n", "        size = (w, h)\n", "\n", "        out = cv2.VideoWriter(os.path.join(CLEAN_VIDEO_OUT + filename_main + '_clean.avi'), cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n", "        for i in range(len(clean_vid_frames)):\n", "            out.write(clean_vid_frames[i])\n", "        out.release()    "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["-----\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Data Science Ethics Checklist\n", "\n", "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n", "\n", "## A. Data Collection\n", " - [ ] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n", " - [ ] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n", " - [ ] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n", " - [ ] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n", "\n", "## B. Data Storage\n", " - [ ] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n", " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n", " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n", "\n", "## C. Analysis\n", " - [ ] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n", " - [ ] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n", " - [ ] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n", " - [ ] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n", " - [ ] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n", "\n", "## D. Modeling\n", " - [ ] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n", " - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n", " - [ ] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n", " - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n", " - [ ] **D.5 Communicate bias**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n", "\n", "## E. Deployment\n", " - [ ] **E.1 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n", " - [ ] **E.2 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n", " - [ ] **E.3 Concept drift**: Do we test and monitor for concept drift to ensure the model remains fair over time?\n", " - [ ] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n", "\n", "*Data Science Ethics Checklist generated with [deon](http://deon.drivendata.org).*\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 4}